# -*- coding: utf-8 -*-
"""Project_472.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f-yuefQ_U2sLtcc-Jr5TJv8XN38chHxF
"""

#Installing pytorch using command given on https://pytorch.org/get-started/locally/

!pip install torch torchvision torchaudio

import torch
import torchvision
from torchvision import transforms, models
from torch.utils.data import DataLoader, Subset
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#transform the dataset we first have to transform the dataset by resizing it and then normalizing it,
#we call Compose function in the transforms library and call Resize and Normalize. This transform attribute will be sent to
# as a parameter to .CIFAR10 and will tranform all the data.
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

#load the data, training data will be True and test data which will be tested will be False in the
#train attribute.

training_data = torchvision.datasets.CIFAR10(root="data", train=True, download=True, transform=transform)
test_data = torchvision.datasets.CIFAR10(root="data", train=False, download=True, transform=transform)

# To make this possible "In this project, you will only use 500 training images and 100 test images per class. Therefore, your first
#task is to load the dataset and use the first 500 training images, and the first 100 test images of each
#class." we need to create a function which will return the indices of the first amount of objects
#inpted. If we want 50 we will but 50 training etc.

def get_Class_indices(dataset, samples_per_class, num_classes=10):
    class_indices = {i: []
                     for i in range(num_classes)}

    for index, (_, label) in enumerate(dataset):
        class_indices[label].append(index)

    subset_indices = []
    for class_id in range(num_classes):
        subset_indices.extend(class_indices[class_id][:samples_per_class])

    return subset_indices

#We call this function inputting 500 training data and 100 training for each class
training_indices = get_Class_indices(training_data, 500)
testing_indices = get_Class_indices(test_data, 100)

#We create a subset of each data
train_subset = Subset(training_data, training_indices)
test_subset = Subset(test_data, testing_indices )


#We load the data by batches, we will create batches of 32 to start off and see its precision
train_loader = DataLoader(train_subset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)

# Feature Extraction using Pre-trained ResNet-18
# This part was generated by GPT
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
resnet18 = models.resnet18(pretrained=True)
resnet18.fc = torch.nn.Identity()  # Remove the final fully connected layer
resnet18 = resnet18.to(device)
resnet18.eval()

def extract_features(loader):
    features = []
    labels = []

    with torch.no_grad():
        for images, targets in loader:
            images = images.to(device)
            outputs = resnet18(images)
            features.append(outputs.cpu().numpy())
            labels.append(targets.numpy())

    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)
    return features, labels


# Here with the function declared we call it to extract features for the training data and
#the features for the testing data

x_train, y_train = extract_features(train_loader)
x_test, y_test = extract_features(test_loader)

# Reduction with PCA
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

pca = PCA(n_components=50)
x_train_pca = pca.fit_transform(x_train)
x_test_pca = pca.transform(x_test)

print("Shape of training data after PCA:", x_train_pca.shape)
print("Shape of test data after PCA:", x_test_pca.shape)

# Gaussian implementation

class Gaussian:
  def __init__(self):
    self.means = None
    self.variances = None
    self.priors = None

  def fit(self, X, y):
    n_classes = len(np.unique(y))
    n_features = X.shape[1]

    self.means = np.zeros((n_classes, n_features))
    self.variances = np.zeros((n_classes, n_features))
    self.priors = np.zeros(n_classes)

    for c in range(n_classes):
            X_c = X[y == c]
            self.means[c, :] = np.mean(X_c, axis=0)
            self.variances[c, :] = np.var(X_c, axis=0) + 1e-6
            self.priors[c] = X_c.shape[0] / X.shape[0]

  def predict(self, X):

        log_likelihoods = self._calculate_log_likelihood(X)
        return np.argmax(log_likelihoods, axis=1)

  def _calculate_log_likelihood(self, X):

        n_samples, n_features = X.shape
        n_classes = self.means.shape[0]
        log_likelihoods = np.zeros((n_samples, n_classes))

        for c in range(n_classes):
            mean = self.means[c, :]
            variance = self.variances[c, :]
            prior = self.priors[c]

            log_gaussian = -0.5 * np.sum(
                np.log(2 * np.pi * variance) + ((X - mean) ** 2) / variance, axis=1
            )
            log_likelihoods[:, c] = log_gaussian + np.log(prior)

        return log_likelihoods

gnb = Gaussian()
gnb.fit(x_train_pca, y_train)

# Predict on the test set
y_pred_impl = gnb.predict(x_test_pca)

# Calculate accuracy
accuracy = np.mean(y_pred_impl == y_test)
print("Test Accuracy:", accuracy)

# Now let's check with the sklearn implentation of GaussianNB algorithm and compare with our implementation

from sklearn.naive_bayes import GaussianNB

GNB = GaussianNB()
GNB.fit(x_train_pca, y_train)

# Predict on the test set
y_pred = GNB.predict(x_test_pca)

# Calculate accuracy
accuracy = np.mean(y_pred == y_test)
print("Test Accuracy:", accuracy)

#Now we can generate the confusion matrix for the bayes implemntaion

#from sklearn import metrics
#from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score
#import seaborn as sns
#import matplotlib.pyplot as plt

#confusion_matrix = metrics.confusion_matrix(y_test, y_pred)

#cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])
#cm_display.plot()
#plt.show()

#Now we can generate the confusion matrix for the bayes implemntaion

from sklearn import metrics
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score
import seaborn as sns
import matplotlib.pyplot as plt

confusion_matrix = metrics.confusion_matrix(y_test, y_pred)


class_names = [str(i) for i in range(10)]
cm_display = metrics.ConfusionMatrixDisplay(
    confusion_matrix=confusion_matrix,
    display_labels=class_names
)
cm_display.plot()
plt.show()

f1_score(y_test, y_pred, average='weighted')

#Present the metrics (accuracy, precision, recall, and F1-measure) of the four ML models and their
#variants.

precision_score = metrics.precision_score(y_test, y_pred, average='weighted')
recall_score = metrics.recall_score(y_test, y_pred, average='weighted')
accuracy_score = metrics.accuracy_score(y_test, y_pred)
f1_score = metrics.f1_score(y_test, y_pred, average='weighted')

print("Precision Score:", precision_score)
print("Recall Score:", recall_score)
print("Accuracy Score:", accuracy_score)
print("F1 Score:", f1_score)
#